{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4. - Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Missing values](missing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data is messy and often contains a lot of missing values. \n",
    "\n",
    "There could be multiple reasons for the missing values but primarily the reason for missingness can be attributed to:\n",
    "\n",
    "| Reason for missing Data | \n",
    "| :-----------: | \n",
    "| Data doesn't exist |\n",
    "| Data not collected due to human error. | \n",
    "| Data deleted accidently |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A guide to handling missing values \n",
    "\n",
    "Please read this tutorial on handling missing values first, before working on dirty data this week: [TUTORIAL](a_guide_to_na.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "import ssl\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Disable SSL verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the provided URL using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.github.com/edwindj/datacleaning/master/data/dirty_iris.csv\"\n",
    "response = requests.get(url, verify=False)\n",
    "data = StringIO(response.text)\n",
    "dirty_iris = pd.read_csv(data, sep=\",\")\n",
    "print(dirty_iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Missing Values\n",
    "\n",
    "Randomly introduce missing values into the dataset to mimic the Python code behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load additional data\n",
    "carseats = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/Carseats.csv\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Introduce missing values in 'Income' column\n",
    "income_missing_indices = np.random.choice(carseats.index, size=20, replace=False)\n",
    "carseats.loc[income_missing_indices, 'Income'] = np.nan\n",
    "\n",
    "# Set another random seed for reproducibility\n",
    "np.random.seed(456)\n",
    "\n",
    "# Introduce missing values in 'Urban' column\n",
    "urban_missing_indices = np.random.choice(carseats.index, size=10, replace=False)\n",
    "carseats.loc[urban_missing_indices, 'Urban'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of highlighting useful information, suggesting conclusions and supporting decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descriptive Statistics](images/ds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times in the beginning we spend hours on handling problems with missing values, logical inconsistencies or outliers in our datasets. In this tutorial we will go through the most popular techniques in data cleansing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the messy dataset `iris`. Originally published at UCI Machine Learning Repository: Iris Data Set, this small dataset from 1936 is often used for testing out machine learning algorithms and visualizations. Each row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\n",
    "\n",
    "Take a look at this dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting NA\n",
    "\n",
    "A missing value, represented by NaN in Python, is a placeholder for a datum of which the type is known but its value isn't. Therefore, it is impossible to perform statistical analysis on data where one or more values in the data are missing. One may choose to either omit elements from a dataset that contain missing values or to impute a value, but missingness is something to be dealt with prior to any analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Descriptive Statistics](images/ds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see that many values in our dataset have status NaN = Not Available? Count (or plot), how many (%) of all 150 rows is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of complete cases (rows without any missing values)\n",
    "complete_cases = dirty_iris.dropna().shape[0]\n",
    "\n",
    "# Calculate the percentage of complete cases\n",
    "percentage_complete = (complete_cases / dirty_iris.shape[0]) * 100\n",
    "\n",
    "print(f\"Number of complete cases: {complete_cases}\")\n",
    "print(f\"Percentage of complete cases: {percentage_complete:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the data contain other special values? If it does, replace them with NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check for special values\n",
    "def is_special(x):\n",
    "    if np.issubdtype(x.dtype, np.number):\n",
    "        return ~np.isfinite(x)\n",
    "    else:\n",
    "        return pd.isna(x)\n",
    "\n",
    "# Apply the function to each column and replace special values with NaN\n",
    "for col in dirty_iris.columns:\n",
    "    dirty_iris[col] = dirty_iris[col].apply(lambda x: np.nan if is_special(pd.Series([x]))[0] else x)\n",
    "\n",
    "# Display summary of the data\n",
    "print(dirty_iris.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking consistency\n",
    "\n",
    "Consistent data are technically correct data that are fit for statistical analysis. They are data in which missing values, special values, (obvious) errors and outliers are either removed, corrected or imputed. The data are consistent with constraints based on real-world knowledge about the subject that the data describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Iris](images/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following background knowledge:\n",
    "\n",
    "-   Species should be one of the following values: setosa, versicolor or virginica.\n",
    "\n",
    "-   All measured numerical properties of an iris should be positive.\n",
    "\n",
    "-   The petal length of an iris is at least 2 times its petal width.\n",
    "\n",
    "-   The sepal length of an iris cannot exceed 30 cm.\n",
    "\n",
    "-   The sepals of an iris are longer than its petals.\n",
    "\n",
    "Define these rules in a separate object 'RULES' and read them into Python. Print the resulting constraint object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rules as functions\n",
    "def check_rules(df):\n",
    "    rules = {\n",
    "        \"Sepal.Length <= 30\": df[\"Sepal.Length\"] <= 30,\n",
    "        \"Species in ['setosa', 'versicolor', 'virginica']\": df[\"Species\"].isin(['setosa', 'versicolor', 'virginica']),\n",
    "        \"Sepal.Length > 0\": df[\"Sepal.Length\"] > 0,\n",
    "        \"Sepal.Width > 0\": df[\"Sepal.Width\"] > 0,\n",
    "        \"Petal.Length > 0\": df[\"Petal.Length\"] > 0,\n",
    "        \"Petal.Width > 0\": df[\"Petal.Width\"] > 0,\n",
    "        \"Petal.Length >= 2 * Petal.Width\": df[\"Petal.Length\"] >= 2 * df[\"Petal.Width\"],\n",
    "        \"Sepal.Length > Petal.Length\": df[\"Sepal.Length\"] > df[\"Petal.Length\"]\n",
    "    }\n",
    "    return rules\n",
    "\n",
    "# Apply the rules to the dataframe\n",
    "rules = check_rules(dirty_iris)\n",
    "\n",
    "# Print the rules\n",
    "for rule, result in rules.items():\n",
    "    print(f\"{rule}: {result.all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to determine how often each rule is broken (violations). Also we can summarize and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rule violations\n",
    "violations = {rule: ~result for rule, result in rules.items()}\n",
    "\n",
    "# Summarize the violations\n",
    "summary = {rule: result.sum() for rule, result in violations.items()}\n",
    "\n",
    "# Print the summary of violations\n",
    "print(\"Summary of Violations:\")\n",
    "for rule, count in summary.items():\n",
    "    print(f\"{rule}: {count} violations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of the data has no errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the violations\n",
    "violation_counts = pd.Series(summary)\n",
    "ax = violation_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Summary of Rule Violations')\n",
    "plt.xlabel('Rules')\n",
    "plt.ylabel('Number of Violations')\n",
    "\n",
    "# Add percentage labels above the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() / len(dirty_iris) * 100:.1f}%', \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', xytext=(0, 10), \n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out which observations have too long sepals using the result of violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rule violations\n",
    "violations = {rule: ~result for rule, result in rules.items()}\n",
    "# Combine violations into a DataFrame\n",
    "violated_df = pd.DataFrame(violations)\n",
    "violated_rows = dirty_iris[violated_df[\"Sepal.Length <= 30\"]]\n",
    "print(violated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find outliers in sepal length using boxplot approach. Retrieve the corresponding observations and look at the other values. Any ideas what might have happened? Set the outliers to NA (or a value that you find more appropiate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for Sepal.Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(dirty_iris['Sepal.Length'].dropna())\n",
    "plt.title('Boxplot of Sepal Length')\n",
    "plt.ylabel('Sepal Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find outliers in Sepal.Length\n",
    "outliers = dirty_iris['Sepal.Length'][np.abs(dirty_iris['Sepal.Length'] - dirty_iris['Sepal.Length'].mean()) > (1.5 * dirty_iris['Sepal.Length'].std())]\n",
    "outliers_idx = dirty_iris.index[dirty_iris['Sepal.Length'].isin(outliers)]\n",
    "\n",
    "# Print the rows with outliers\n",
    "print(\"Outliers:\")\n",
    "print(dirty_iris.loc[outliers_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all seem to be too big... may they were measured in mm i.o cm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the outliers (assuming they were measured in mm instead of cm)\n",
    "dirty_iris.loc[outliers_idx, ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']] /= 10\n",
    "\n",
    "# Summary of the adjusted data\n",
    "print(\"Summary of adjusted data:\")\n",
    "print(dirty_iris.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that simple boxplot shows an extra outlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Species', y='Sepal.Length', data=dirty_iris)\n",
    "plt.title('Boxplot of Sepal Length by Species')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Sepal Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting\n",
    "\n",
    "Replace non positive values from Sepal.Width with NA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correction rule\n",
    "def correct_sepal_width(df):\n",
    "    df.loc[(~df['Sepal.Width'].isna()) & (df['Sepal.Width'] <= 0), 'Sepal.Width'] = np.nan\n",
    "    return df\n",
    "\n",
    "# Apply the correction rule to the dataframe\n",
    "mydata_corrected = correct_sepal_width(dirty_iris)\n",
    "\n",
    "# Print the corrected dataframe\n",
    "print(mydata_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all erroneous values with NA using (the result of) localizeErrors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the rules to the dataframe\n",
    "rules = check_rules(dirty_iris)\n",
    "violations = {rule: ~result for rule, result in rules.items()}\n",
    "violated_df = pd.DataFrame(violations)\n",
    "\n",
    "# Localize errors and set them to NA\n",
    "for col in violated_df.columns:\n",
    "    dirty_iris.loc[violated_df[col], col.split()[0]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA's pattern detection\n",
    "\n",
    "Here we are going to use **missingno** library to diagnose the missingness pattern for the 'dirty_iris' dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Plot (msno.matrix):\n",
    "\n",
    "This visualization shows which values are missing in each column. Each bar represents a column, and white spaces in the bars indicate missing values.\n",
    "\n",
    "If you see many white spaces in one column, it means that column has a lot of missing data.\n",
    "If the white spaces are randomly scattered, the missing data might be random. \n",
    "If they are clustered in specific areas, it might indicate a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dirty_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Plot (msno.heatmap):\n",
    "\n",
    "This visualization shows the correlations between missing values in different columns.\n",
    "If two columns have a high correlation (dark colors), it means that if one column has missing values, the other column is also likely to have missing values.\n",
    "\n",
    "Low correlations (light colors) indicate that missing values in one column are not related to missing values in another column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(dirty_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram Plot (msno.dendrogram):\n",
    "\n",
    "This visualization groups columns based on the similarity of their missing data patterns.\n",
    "Columns that are close to each other in the dendrogram have similar patterns of missing data.\n",
    "\n",
    "This can help identify groups of columns that have similar issues with missing data.\n",
    "\n",
    "Based on these visualizations, we can identify which columns have the most missing data, whether the missing data is random or patterned, and which columns have similar patterns of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(dirty_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on the dendrogram plot, we can interpret the pattern of missing data in the \"dirty iris\" dataset as follows:*\n",
    "\n",
    "**Grouping of Columns:**\n",
    "\n",
    "The dendrogram shows that the columns \"Species\" and \"Petal.Width\" are grouped together, indicating that they have similar patterns of missing data.\n",
    "\n",
    "Similarly, \"Sepal.Width\" and \"Petal.Length\" are grouped together, suggesting they also share a similar pattern of missing data.\n",
    "\n",
    "\"Sepal.Length\" is somewhat separate from the other groups, indicating it has a different pattern of missing data compared to the other columns.\n",
    "\n",
    "**Pattern of Missing Data:**\n",
    "\n",
    "The grouping suggests that missing data in \"Species\" is likely to be associated with missing data in \"Petal.Width\".\n",
    "\n",
    "Similarly, missing data in \"Sepal.Width\" is likely to be associated with missing data in \"Petal.Length\".\n",
    "\n",
    "\"Sepal.Length\" appears to have a distinct pattern of missing data that is not strongly associated with the other columns.\n",
    "\n",
    "*From this dendrogram, we can infer that the missing data is not completely random. Instead, there are specific patterns where certain columns tend to have missing data together. This indicates a systematic pattern of missing data rather than a purely random one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing NA's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation is the process of estimating or deriving values for fields where data is missing. There is a vast body of literature on imputation methods and it goes beyond the scope of this tutorial to discuss all of them.\n",
    "\n",
    "There is no one single best imputation method that works in all cases. The imputation model of choice depends on what auxiliary information is available and whether there are (multivariate) edit restrictions on the data to be imputed. \n",
    "\n",
    "The availability of Python software for imputation under edit restrictions is, to our best knowledge, limited. However, a viable strategy for imputing numerical data is to first impute missing values without restrictions, and then minimally adjust the imputed values so that the restrictions are obeyed. Separately, these methods are available in Python.\n",
    "\n",
    "We can mention several approaches to imputation:\n",
    "\n",
    "1.  For the **quantitative** variables:\n",
    "\n",
    "-   imputing by **mean**/**median**/**mode**\n",
    "\n",
    "-   **hotdeck** imputation\n",
    "\n",
    "-   **KNN** -- K-nearest-neighbors approach\n",
    "\n",
    "-   **RPART** -- random forests multivariate approach\n",
    "\n",
    "-   **mice** - Multivariate Imputation by Chained Equations approach\n",
    "\n",
    "2.  For the **qualitative** variables:\n",
    "\n",
    "-   imputing by **mode**\n",
    "\n",
    "-   **RPART** -- random forests multivariate approach\n",
    "\n",
    "-   **mice** - Multivariate Imputation by Chained Equations approach\n",
    "\n",
    "    ... and many others. Please read the theoretical background if you are interested in those techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.*** Use ***kNN*** imputation ('sklearn' package) to impute all missing values. The KNNImputer from sklearn requires all data to be numeric. Since our dataset contains categorical data (e.g., the Species column), you need to handle these columns separately. One approach is to use one-hot encoding for categorical variables before applying the imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Replace infinite values with NaN\n",
    "dirty_iris.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Separate numeric and categorical columns\n",
    "numeric_cols = dirty_iris.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = dirty_iris.select_dtypes(exclude=[np.number]).columns\n",
    "# One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "encoded_categorical = pd.DataFrame(encoder.fit_transform(dirty_iris[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Combine numeric and encoded categorical columns\n",
    "combined_data = pd.concat([dirty_iris[numeric_cols], encoded_categorical], axis=1)\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Perform kNN imputation\n",
    "imputed_data = imputer.fit_transform(combined_data)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=combined_data.columns)\n",
    "\n",
    "# Decode the one-hot encoded columns back to original categorical columns\n",
    "decoded_categorical = pd.DataFrame(encoder.inverse_transform(imputed_df[encoded_categorical.columns]), columns=categorical_cols)\n",
    "\n",
    "# Combine numeric and decoded categorical columns\n",
    "final_imputed_data = pd.concat([imputed_df[numeric_cols], decoded_categorical], axis=1)\n",
    "\n",
    "# Print the imputed data\n",
    "print(final_imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Finally, we sometimes encounter the situation where we have problems with skewed distributions or we just want to transform, recode or perform discretization. Let's review some of the most popular transformation methods.\n",
    "\n",
    "First, standardization (also known as normalization):\n",
    "\n",
    "-   **Z-score** approach - standardization procedure, using the formula: $z=\\frac{x-\\mu}{\\sigma}$ where $\\mu$ = mean and $\\sigma$ = standard deviation. Z-scores are also known as standardized scores; they are scores (or data values) that have been given a common *standard*. This standard is a mean of zero and a standard deviation of 1.\n",
    "\n",
    "-   **minmax** approach - An alternative approach to Z-score normalization (or standardization) is the so-called MinMax scaling (often also simply called \"normalization\" - a common cause for ambiguities). In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers. If you would like to perform MinMax scaling - simply substract minimum value and divide it by range:$(x-min)/(max-min)$\n",
    "\n",
    "In order to solve problems with very skewed distributions we can also use several types of simple transformations:\n",
    "\n",
    "-   log\n",
    "-   log+1\n",
    "-   sqrt\n",
    "-   x\\^2\n",
    "-   x\\^3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.*** Standardize incomes and present the transformed distribution of incomes on boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "Sometimes we just would like to perform so called 'binning' procedure to be able to analyze our categorical data, to compare several categorical variables, to construct statistical models etc. Thanks to the 'binning' function we can transform quantitative variables into categorical using several methods:\n",
    "\n",
    "-   **quantile** - automatic binning by quantile of its distribution\n",
    "\n",
    "-   **equal** - binning to achieve fixed length of intervals\n",
    "\n",
    "-   **pretty** - a compromise between the 2 mentioned above\n",
    "\n",
    "-   **kmeans** - categorization using the K-Means algorithm\n",
    "\n",
    "-   **bclust** - categorization using the bagged clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Using quantile approach perform binning of the variable 'Income'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Recode the original distribution of incomes using fixed length of intervals and assign them labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of statistical modeling (i.e. credit scoring purposes) - we need to be aware of the fact, that the ***optimal*** discretization of the original distribution must be achieved. The '*binning_by*' function comes with some help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal binning with binary target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Perform discretization of the variable 'Advertising' using optimal binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning import OptimalBinning\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a variable to discretize and the binary target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = \"mean radius\"\n",
    "x = df[variable].values\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and instantiate an OptimalBinning object class. We pass the variable name, its data type, and a solver, in this case, we choose the constraint programming solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optb = OptimalBinning(name=variable, dtype=\"numerical\", solver=\"cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the optimal binning object with arrays x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optb.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if an optimal solution has been found via the status attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optb.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retrieve the optimal split points via the splits attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optb.splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binning table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal binning algorithms return a binning table; a binning table displays the binned data and several metrics for each bin. Class OptimalBinning returns an object BinningTable via the binning_table attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_table = optb.binning_table\n",
    "\n",
    "type(binning_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binning_table is instantiated, but not built. Therefore, the first step is to call the method build, which returns a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_table.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s describe the columns of this binning table:\n",
    "\n",
    "Bin: the intervals delimited by the optimal split points.  \n",
    "Count: the number of records for each bin.  \n",
    "Count (%): the percentage of records for each bin.  \n",
    "Non-event: the number of non-event records (ð‘¦=0) for each bin.  \n",
    "Event: the number of event records (ð‘¦=1) for each bin.  \n",
    "Event rate: the percentage of event records for each bin.  \n",
    "WoE: the Weight-of-Evidence for each bin.  \n",
    "IV: the Information Value (also known as Jeffreyâ€™s divergence) for each bin.  \n",
    "JS: the Jensen-Shannon divergence for each bin.  \n",
    "The last row shows the total number of records, non-event records, event records, and IV and JS.    \n",
    "\n",
    "You can use the method plot to visualize the histogram and WoE or event rate curve. Note that the Bin ID corresponds to the binning table index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_table.plot(metric=\"woe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that WoE is inversely related to the event rate, i.e., a monotonically ascending event rate ensures a monotonically descending WoE and vice-versa. We will see more monotonic trend options in the advanced tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more here: [https://gnpalencia.org/optbinning/tutorials/tutorial_binary.html](https://gnpalencia.org/optbinning/tutorials/tutorial_binary.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with 'missingno' library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Wdvwer7h-8w?si=pVqCbOXb4CaCsmnJ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.** Your turn! \n",
    "\n",
    "Work with the 'carseats' dataset, find the best way to perform full diagnostic (dirty data, outliers, missing values). Fix problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases without Nan: 370\n",
      "Percentage of cases without Nan: 92.5\n"
     ]
    }
   ],
   "source": [
    "#finding missing values\n",
    "without_NaN = carseats.dropna().shape[0]\n",
    "percentage_of_complete = (without_NaN/carseats.shape[0])*100\n",
    "print(f\"Number of cases without Nan: {without_NaN}\")\n",
    "print(f\"Percentage of cases without Nan: {percentage_of_complete}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting other special values\n",
    "def is_special(x):\n",
    "    if np.issubdtype(x.dtype, np.number):\n",
    "        return ~np.isfinite(x)\n",
    "    else:\n",
    "        return pd.isna(x)\n",
    "\n",
    "# Apply the function to each column and replace special values with NaN\n",
    "for col in carseats.columns:\n",
    "    carseats[col] = carseats[col].apply(lambda x: np.nan if is_special(pd.Series([x]))[0] else x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting values with rules\n",
    "def check_rules(df):\n",
    "    rules = {\n",
    "        \"Sales > 0\": df[\"Sales\"].isna() | (df[\"Sales\"] > 0),\n",
    "        \"Age > 0\": df[\"Age\"] > 18,\n",
    "        \"Age < 120\": df[\"Age\"] < 120,\n",
    "        \"ShelveLoc in ['Good', 'Bad', 'Medium']\": df[\"ShelveLoc\"].isin(['Good', 'Medium', 'Bad']),\n",
    "        \"Urban in ['Yes', 'No']\": df[\"Urban\"].isna() | df[\"Urban\"].isin(['Yes', 'No']),\n",
    "        \"US in ['Yes', 'No']\": df[\"US\"].isin(['Yes', 'No']),\n",
    "        \"Education > 0\": df[\"Education\"] > 0,\n",
    "        \"Education > 30\": df[\"Education\"] < 30\n",
    "    }\n",
    "    return rules\n",
    "\n",
    "\n",
    "\n",
    "def correct_sales(df):\n",
    "    df.loc[(~df['Sales'].isna()) & (df['Sales'] <= 0), 'Sales'] = np.nan\n",
    "    return df\n",
    "\n",
    "# correct Urban\n",
    "def correct_urban(df):\n",
    "    df.loc[(~df['Urban'].isin(['Yes', 'No']) & ~df['Urban'].isna()), 'Urban'] = np.nan\n",
    "    return df\n",
    "    \n",
    "\n",
    "carseats = correct_sales(carseats)\n",
    "carseats = correct_urban(carseats)\n",
    "\n",
    "rules = check_rules(carseats)\n",
    "\n",
    "for rule, result in rules.items():\n",
    "    print(f\"{rule}: {result.all()}\")\n",
    "\n",
    "# \n",
    "\n",
    "# Check for rule violations\n",
    "violations = {rule: ~result for rule, result in rules.items()}\n",
    "\n",
    "# Summarize the violations\n",
    "summary = {rule: result.sum() for rule, result in violations.items()}\n",
    "\n",
    "# Print the summary of violations\n",
    "\n",
    "print(\"Summary of Violations:\")\n",
    "for rule, count in summary.items():\n",
    "    print(f\"{rule}: {count} violations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
